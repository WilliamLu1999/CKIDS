{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a732d551-ea2e-4253-ae9f-2df7cc66cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook contains all the correct codes for the screenshot classifier. Outputs not included otherwise the file size is too large to upload on Github.\n",
    "# William Lu; Zexun Yao\n",
    "# CKIDS Misinformation Diffusion\n",
    "# Nov 25th 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32e0bc-d684-4363-8f3d-18f1afd2b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a53b1f-862b-4b94-a314-25ba99867a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_files = os.listdir('/project/ll_774_951/uk_ru/twitter/training_set/')         # æ”¹\n",
    "img_files = list(filter(lambda x: x != 'training_set', img_files))\n",
    "def train_path(p): return f\"/project/ll_774_951/uk_ru/twitter/training_set/{p}\"\n",
    "img_files = list(map(train_path, img_files))\n",
    "\n",
    "print(\"total training images\", len(img_files))\n",
    "print(\"First item\", img_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f1dc7-30c5-4eb3-9f1a-e0736c6a49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(img_files)\n",
    "\n",
    "train = img_files[:4493] # increase the number of training\n",
    "test = img_files[4493:] # 20% of all images\n",
    "\n",
    "print(\"train size\", len(train))\n",
    "print(\"test size\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f24d9-ed85-44db-8b34-d1fdf1fa6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e2d22a-e12c-4575-8e94-165a89a18d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweetdataset(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        super().__init__()\n",
    "        self.paths = image_paths\n",
    "        self.len = len(self.paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return self.len\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        path = self.paths[index]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        label = 0 if 'anyuser' in path else 1\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb694e-fce5-426b-b12f-528c79a833ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tweetdataset(train, transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=100)\n",
    "print(len(train_ds), len(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf9870a-20b4-4bec-bdab-02439098a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tweetdataset(test, transform)\n",
    "test_dl = DataLoader(test_ds, batch_size=100)\n",
    "print(len(test_ds), len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf950821-da59-4aa4-981f-c402a8eff3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweetscreenshot(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # onvolutional layers (3,16,32)\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size=(5, 5), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(5, 5), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
    "\n",
    "        # conected layers\n",
    "        self.fc1 = nn.Linear(in_features= 64 * 6 * 6, out_features=500)\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=50)\n",
    "        self.fc3 = nn.Linear(in_features=50, out_features=2)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2)\n",
    "\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2)\n",
    "\n",
    "        X = F.relu(self.conv3(X))\n",
    "        X = F.max_pool2d(X, 2)\n",
    "\n",
    "        X = X.view(X.shape[0], -1)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f42d10e-5dae-41a3-b74c-4587204322e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tweetscreenshot()\n",
    "losses = []\n",
    "accuracies = []\n",
    "epoches = 3\n",
    "start = time.time()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ba2f27-c938-4e34-bf68-d11ec8ed8c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epoches):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "\n",
    "    for X, y in train_dl:\n",
    "        preds = model(X)\n",
    "        loss = loss_fn(preds, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        accuracy = ((preds.argmax(dim=1) == y).float().mean())\n",
    "        epoch_accuracy += accuracy\n",
    "        epoch_loss += loss\n",
    "        print('.', end='', flush=True)\n",
    "\n",
    "    epoch_accuracy = epoch_accuracy/len(train_dl)\n",
    "    accuracies.append(epoch_accuracy)\n",
    "    epoch_loss = epoch_loss / len(train_dl)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        test_epoch_loss = 0\n",
    "        test_epoch_accuracy = 0\n",
    "\n",
    "        for test_X, test_y in test_dl:\n",
    "\n",
    "            test_preds = model(test_X)\n",
    "            test_loss = loss_fn(test_preds, test_y)\n",
    "\n",
    "            test_epoch_loss += test_loss            \n",
    "            test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
    "            test_epoch_accuracy += test_accuracy\n",
    "\n",
    "        test_epoch_accuracy = test_epoch_accuracy/len(test_dl)\n",
    "        test_epoch_loss = test_epoch_loss / len(test_dl)\n",
    "\n",
    "        print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ab27e-b1ed-4e29-95b2-6965afdc5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the first model complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b484a-408b-4996-8bce-024095304ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files_step1 = os.listdir('/project/ll_774_951/uk_ru/twitter/test_set/')\n",
    "test_files_step1 = list(filter(lambda x: x != 'test_set', test_files_step1))#old: test_set, new: step3_04?\n",
    "def test_path_step1(p): return f\"/project/ll_774_951/uk_ru/twitter/test_set/{p}\"\n",
    "test_files_step1 = list(map(test_path_step1, test_files_step1))[:20000]\n",
    "\n",
    "class Test_step1(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        super().__init__()\n",
    "        self.paths = image_paths\n",
    "        self.len = len(self.paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return self.len\n",
    "\n",
    "#     def __getitem__(self, index): \n",
    "#         path = self.paths[index]\n",
    "#         image = Image.open(path).convert('RGB')\n",
    "#         image = self.transform(image)\n",
    "#         fileid = path.split('/')[-1].split('.')[0]\n",
    "#         return (image, fileid)\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        path = self.paths[index]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        fileid = path.split('/')[-1].split('.')[0]\n",
    "        return (image, fileid)\n",
    "    \n",
    "test_ds_step1 = Test_step1(test_files_step1, transform)\n",
    "test_dl_step1 = DataLoader(test_ds_step1, batch_size=100)\n",
    "len(test_ds_step1), len(test_dl_step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f895bf9-1a6e-4b1f-bb70-7e9b598256c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "screenshot_probs_step1 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, fileid in test_dl_step1:\n",
    "        preds = model(X)\n",
    "        preds_list = F.softmax(preds, dim=1)[:, 1].tolist()\n",
    "        screenshot_probs_step1 += list(zip(list(fileid), preds_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8562e9a2-b958-40d3-b716-d56146d1c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some screenshot images\n",
    "counter_step1 = 0 \n",
    "for img, probs in zip(test_files_step1, screenshot_probs_step1):\n",
    "    pil_im = Image.open(img, 'r')\n",
    "    label = \"non-screenshot\" if probs[1] > 0.5 else \"screenshot\"\n",
    "    title = \"prob of non-screenshot: \" + str(probs[1]) + \" Classified as: \" + label\n",
    "    if (label == \"screenshot\") and counter_step1<=100:\n",
    "        counter_step1+=1\n",
    "        #count += 1\n",
    "        plt.figure()\n",
    "        plt.imshow(pil_im)\n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99760e4f-d020-4f4c-9de3-0afff5a9da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some non-screenshot images\n",
    "counter_step11 = 0 \n",
    "for img, probs in zip(test_files_step1, screenshot_probs_step1):\n",
    "    pil_im = Image.open(img, 'r')\n",
    "    label = \"non-screenshot\" if probs[1] > 0.5 else \"screenshot\"\n",
    "    title = \"prob of non-screenshot: \" + str(probs[1]) + \" Classified as: \" + label\n",
    "    if (label == \"non-screenshot\") and counter_step11<=100:\n",
    "        counter_step11+=1\n",
    "        #count += 1\n",
    "        plt.figure()\n",
    "        plt.imshow(pil_im)\n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3e030-802d-4730-84eb-eb968189e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_all_prediction(model, loader):\n",
    "    preds = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "        batch_predictions = model(images)\n",
    "        preds = torch.cat((preds, batch_predictions), dim = 0)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337c5db-5b45-4ef7-bdc1-fad734aa7926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebbdf05-017d-4f12-ae1a-71fcd552e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "train_preds = get_all_prediction(model, train_dl)\n",
    "label_dict = {\n",
    " 'Screenshot', 'Non-Screenshot'\n",
    "}\n",
    "\n",
    "train_label=[]\n",
    "for i in range(0,len(train_ds)):\n",
    "    train_label.append(train_ds.__getitem__(i)[1])\n",
    "train_label= torch.FloatTensor(train_label)\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "\n",
    "\n",
    "plt.figure(figsize=(50,50))\n",
    "a=plot_confusion_matrix(train_label, train_preds.argmax(dim=1), classes=label_dict,\n",
    "                      title='Confusion matrix')\n",
    "print(a)\n",
    "plt.show()\n",
    "\n",
    "print('Accuracy:', accuracy_score(train_label, train_preds.argmax(dim=1)))\n",
    "print('F1:', f1_score(train_label, train_preds.argmax(dim=1), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99eabe-5fcd-4cf8-a795-3f0fb32ccae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('Recall:', recall_score(train_label, train_preds.argmax(dim=1)))\n",
    "print('Roc-Auc:', roc_auc_score(train_label, train_preds.argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeaf7f9-0a03-4291-b2c3-82adf77e65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_step1_cm = tweetdataset(test_files_step1, transform)\n",
    "test_dl_step1_cm = DataLoader(test_ds_step1_cm, batch_size=100)\n",
    "print(len(test_ds_step1_cm), len(test_dl_step1_cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871054d-495f-477c-8606-f21109d62c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "test_preds_step1 = get_all_prediction(model, test_dl_step1_cm)\n",
    "label_dict = {\n",
    " 'Screenshot', 'Non-Screenshot'\n",
    "}\n",
    "\n",
    "test_label_step1=[]\n",
    "for i in range(0,len(test_ds_step1_cm)):\n",
    "    test_label_step1.append(test_ds_step1_cm.__getitem__(i)[1])\n",
    "test_label_step1= torch.FloatTensor(test_label_step1)\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "\n",
    "\n",
    "plt.figure(figsize=(50,50))\n",
    "b=plot_confusion_matrix(test_label_step1, test_preds_step1.argmax(dim=1), classes=label_dict,\n",
    "                      title='Confusion matrix for the step1 test data')\n",
    "print(b)\n",
    "plt.show()\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_label_step1, test_preds_step1.argmax(dim=1)))\n",
    "print('F1:', f1_score(test_label_step1, test_preds_step1.argmax(dim=1), average='weighted'))\n",
    "print('Recall:', recall_score(test_label_step1, test_preds_step1.argmax(dim=1)))\n",
    "print('Roc-Auc:', roc_auc_score(test_label_step1, test_preds_step1.argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7462a-c26d-4380-9191-51869cc7213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# Up till here are step1. In other words, the result on the testing data.\n",
    "# Below is step2. In other words, train a new model, called model2, on both the training and testing data\n",
    "# from above. Manually check 200 predictions of each category to find the accuracy.\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7200edb9-a38a-4688-8062-2f196eac01f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_files_step2 = os.listdir('/project/ll_774_951/uk_ru/twitter/detection3/detectionss/')         # æ”¹\n",
    "img_files_step2 = list(filter(lambda x: x != 'detectionss', img_files_step2))\n",
    "def train_path_step2(p): return f\"/project/ll_774_951/uk_ru/twitter/detection3/detectionss/{p}\"\n",
    "img_files_step2 = list(map(train_path_step2, img_files_step2))\n",
    "\n",
    "print(\"total training images\", len(img_files_step2))\n",
    "print(\"First item\", img_files_step2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5103e55e-d659-4a8b-a8f0-f3a213b84b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(img_files_step2)\n",
    "\n",
    "train_step2 = img_files_step2[:5615] # increase the number of training\n",
    "test_step2 = img_files_step2[5615:] # 20% of all images\n",
    "\n",
    "print(\"train size\", len(train_step2))\n",
    "print(\"test size\", len(test_step2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627b054-4ae7-4113-a1f4-8927ed42a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_step2 = tweetdataset(train_step2, transform)\n",
    "train_dl_step2 = DataLoader(train_ds_step2, batch_size=100)\n",
    "print(len(train_ds_step2), len(train_dl_step2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d9ecd9-8be9-4a40-b84c-f0864cf4cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_step2 = tweetdataset(test_step2, transform)\n",
    "test_dl_step2 = DataLoader(test_ds_step2, batch_size=100)\n",
    "print(len(test_ds_step2), len(test_dl_step2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bad0e2-9d02-447d-b192-931b86df0831",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tweetscreenshot()\n",
    "\n",
    "losses2 = []\n",
    "accuracies2 = []\n",
    "epoches2 = 3\n",
    "start2 = time.time()\n",
    "loss_fn2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b0edf-5092-4333-a108-5bb8cd99c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epoches2):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "\n",
    "    for X, y in train_dl_step2:\n",
    "        preds = model2(X)\n",
    "        loss = loss_fn2(preds, y)\n",
    "\n",
    "        optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        accuracy = ((preds.argmax(dim=1) == y).float().mean())\n",
    "        epoch_accuracy += accuracy\n",
    "        epoch_loss += loss\n",
    "        print('.', end='', flush=True)\n",
    "\n",
    "    epoch_accuracy = epoch_accuracy/len(train_dl_step2)\n",
    "    accuracies.append(epoch_accuracy)\n",
    "    epoch_loss = epoch_loss / len(train_dl_step2)\n",
    "    losses2.append(epoch_loss)\n",
    "\n",
    "    print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        test_epoch_loss = 0\n",
    "        test_epoch_accuracy = 0\n",
    "\n",
    "        for test_X, test_y in test_dl_step2:\n",
    "\n",
    "            test_preds = model2(test_X)\n",
    "            test_loss = loss_fn2(test_preds, test_y)\n",
    "\n",
    "            test_epoch_loss += test_loss            \n",
    "            test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
    "            test_epoch_accuracy += test_accuracy\n",
    "\n",
    "        test_epoch_accuracy = test_epoch_accuracy/len(test_dl_step2)\n",
    "        test_epoch_loss = test_epoch_loss / len(test_dl_step2)\n",
    "\n",
    "        print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5571e180-78f3-46f8-adc9-e1a92bc396c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = os.listdir('/project/ll_774_951/uk_ru/twitter/step3_04/')\n",
    "test_files = list(filter(lambda x: x != 'step3_04', test_files))#old: test_set, new: step3_04?\n",
    "def test_path(p): return f\"/project/ll_774_951/uk_ru/twitter/step3_04/{p}\"\n",
    "test_files = list(map(test_path, test_files))[:20000]\n",
    "\n",
    "class Test(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        super().__init__()\n",
    "        self.paths = image_paths\n",
    "        self.len = len(self.paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return self.len\n",
    "\n",
    "#     def __getitem__(self, index): \n",
    "#         path = self.paths[index]\n",
    "#         image = Image.open(path).convert('RGB')\n",
    "#         image = self.transform(image)\n",
    "#         fileid = path.split('/')[-1].split('.')[0]\n",
    "#         return (image, fileid)\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        path = self.paths[index]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        fileid = path.split('/')[-1].split('.')[0]\n",
    "        return (image, fileid)\n",
    "    \n",
    "test_ds2 = Test(test_files, transform)\n",
    "test_dl2 = DataLoader(test_ds2, batch_size=100)\n",
    "len(test_ds2), len(test_dl2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc4319-cb3b-432a-a130-bdf7d95a41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "screenshot_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, fileid in test_dl2:\n",
    "        preds = model(X)\n",
    "        preds_list = F.softmax(preds, dim=1)[:, 1].tolist()\n",
    "        screenshot_probs += list(zip(list(fileid), preds_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c8682-df19-4358-85bf-64bc8af8b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some screenshot images; first 50 images\n",
    "# test data: random images from the web\n",
    "counter = 0\n",
    "#count = 0 \n",
    "for img, probs in zip(test_files, screenshot_probs):\n",
    "    pil_im = Image.open(img, 'r')\n",
    "    label = \"non-screenshot\" if probs[1] > 0.5 else \"screenshot\"\n",
    "    title = \"prob of non-screenshot: \" + str(probs[1]) + \" Classified as: \" + label\n",
    "    if (label == \"screenshot\") and counter<=200:\n",
    "        counter+=1\n",
    "        #count += 1\n",
    "        plt.figure()\n",
    "        plt.imshow(pil_im)\n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdb59a-62f3-4a6d-8b0d-03b2b73d3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some screenshot images; second 50 images\n",
    "# test data: random images from the web\n",
    "zz = 0 \n",
    "for img, probs in zip(test_files[1000:], screenshot_probs[1000:]):\n",
    "    pil_im = Image.open(img, 'r')\n",
    "    label = \"non-screenshot\" if probs[1] > 0.5 else \"screenshot\"\n",
    "    title = \"prob of non-screenshot: \" + str(probs[1]) + \" Classified as: \" + label\n",
    "    if (label == \"screenshot\") and zz<=100:\n",
    "        zz += 1\n",
    "        plt.figure()\n",
    "        plt.imshow(pil_im)\n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f50ed0-6a9c-4831-964c-47d47d950036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some non-screenshot images; first 50 images\n",
    "# test data: random images from the web\n",
    "qq = 0 \n",
    "for img, probs in zip(test_files, screenshot_probs):\n",
    "    pil_im = Image.open(img, 'r')\n",
    "    label = \"non-screenshot\" if probs[1] > 0.5 else \"screenshot\"\n",
    "    title = \"prob of non-screenshot: \" + str(probs[1]) + \" Classified as: \" + label\n",
    "    if (label == \"non-screenshot\") and qq<=200:\n",
    "        qq += 1\n",
    "        plt.figure()\n",
    "        plt.imshow(pil_im)\n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b3c4b-8a27-41b7-ae89-b63f08010582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some non-screenshot images; second 50 images\n",
    "# test data: random images from the web\n",
    "qqq = 0 \n",
    "for img, probs in zip(test_files[1000:], screenshot_probs[1000:]):\n",
    "    pil_im = Image.open(img, 'r')\n",
    "    label = \"non-screenshot\" if probs[1] > 0.5 else \"screenshot\"\n",
    "    title = \"prob of non-screenshot: \" + str(probs[1]) + \" Classified as: \" + label\n",
    "    if (label == \"non-screenshot\") and qqq<=100:\n",
    "        qqq += 1\n",
    "        plt.figure()\n",
    "        plt.imshow(pil_im)\n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29962169-d5be-4599-a513-2390850ec080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test statistics for testing random images on the web\n",
    "total2 = 200\n",
    "TP2 = 53\n",
    "FP2 = 47\n",
    "TN2 = 99\n",
    "FN2 = 1\n",
    "Recall2 = TP2/(TP2+FN2)\n",
    "Precision2 = TP2/(TP2+FP2)\n",
    "Accuracy2 = (TP2+TN2)/(TP2+TN2+FP2+FN2)\n",
    "F12 = 2*((Precision2*Recall2)/(Precision2+Recall2))\n",
    "print('Accuracy:',Accuracy2)\n",
    "print('F1',F12)\n",
    "print('Recall',Recall2)\n",
    "print('Precision',Precision2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1c2ac-6ab7-4efe-8e78-4621f1c18085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Below are for testing kaggle data screenshot. Step 3.\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcedcd4f-76e2-4db7-88ce-2c228f4d5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files_k = os.listdir('/project/ll_774_951/uk_ru/twitter/Twitter_kaggle_screenshot/')\n",
    "test_files_k = list(filter(lambda x: x != 'Twitter_kaggle_screenshot', test_files_k)) # test set kaggle\n",
    "def test_path_k(p): return f\"/project/ll_774_951/uk_ru/twitter/Twitter_kaggle_screenshot/{p}\"\n",
    "test_files_k = list(map(test_path_k, test_files_k))\n",
    "\n",
    "class Test(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        super().__init__()\n",
    "        self.paths = image_paths\n",
    "        self.len = len(self.paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return self.len\n",
    "\n",
    "#     def __getitem__(self, index): \n",
    "#         path = self.paths[index]\n",
    "#         image = Image.open(path).convert('RGB')\n",
    "#         image = self.transform(image)\n",
    "#         fileid = path.split('/')[-1].split('.')[0]\n",
    "#         return (image, fileid)\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        path = self.paths[index]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        fileid = path.split('/')[-1].split('.')[0]\n",
    "        return (image, fileid)\n",
    "    \n",
    "test_dsk = Test(test_files_k, transform)\n",
    "test_dlk = DataLoader(test_dsk, batch_size=100)\n",
    "len(test_dsk), len(test_dlk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4651bf45-2a66-4e91-aece-03680df8f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "screenshot_probs_k = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, fileid in test_dlk:\n",
    "        preds = model2(X)\n",
    "        preds_list = F.softmax(preds, dim=1)[:, 1].tolist()\n",
    "        screenshot_probs_k += list(zip(list(fileid), preds_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da7b7d-fa6b-489f-8aa2-d3af1ac5b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0 \n",
    "for img, probs in zip(test_files_k, screenshot_probs_k):\n",
    "    pil_im = Image.open(img, 'r')\n",
    "    label = \"non-screenshot\" if probs[1] > 0.5 else \"screenshot\"\n",
    "    title = \"prob of non-screenshot: \" + str(probs[1]) + \" Classified as: \" + label\n",
    "    if (label == \"non-screenshot\"):\n",
    "        num+=1\n",
    "        plt.figure()\n",
    "        plt.imshow(pil_im)\n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "print('success')\n",
    "print('Number of images classified as non-screenshot',num) # 162 here, which is a bad result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55859fc4-8d50-4591-8834-f05c10a85194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible improvement for the classifier using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248812a5-e93b-47d1-8378-6430944562c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "#https://medium.com/dataseries/k-fold-cross-validation-with-pytorch-and-sklearn-d094aa00105f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae2fcbf-6065-4f88-962b-70ad5224045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = tweetscreenshot()\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "    history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_correct=train_epoch(model,device,train_loader,criterion,optimizer)\n",
    "        test_loss, test_correct=valid_epoch(model,device,test_loader,criterion)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "        test_loss = test_loss / len(test_loader.sampler)\n",
    "        test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "\n",
    "        print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                             num_epochs,\n",
    "                                                                                                             train_loss,\n",
    "                                                                                                             test_loss,\n",
    "                                                                                                             train_acc,\n",
    "                                                                                                             test_acc))\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "\n",
    "    foldperf['fold{}'.format(fold+1)] = history  \n",
    "\n",
    "best_model = torch.save(model,'k_cross_CNN.pt') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweet capture",
   "language": "python",
   "name": "tweet_capture_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
