{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training images 5616\n",
      "First item /Users/William/Downloads/detection/training_set/@anyuser_1495977824922275840_tweetcapture.png\n"
     ]
    }
   ],
   "source": [
    "img_files = os.listdir('/Users/William/Downloads/detection/training_set/')         # 改\n",
    "img_files = list(filter(lambda x: x != 'training_set', img_files))\n",
    "def train_path(p): return f\"/Users/William/Downloads/detection/training_set/{p}\"\n",
    "img_files = list(map(train_path, img_files))\n",
    "\n",
    "print(\"total training images\", len(img_files))\n",
    "print(\"First item\", img_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 4493\n",
      "test size 1123\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(img_files)\n",
    "\n",
    "train = img_files[:4493]  # 改\n",
    "test = img_files[4493:]\n",
    "\n",
    "print(\"train size\", len(train))\n",
    "print(\"test size\", len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweetdataset(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        super().__init__()\n",
    "        self.paths = image_paths\n",
    "        self.len = len(self.paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return self.len\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        path = self.paths[index]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        label = 0 if 'anyuser' in path else 1\n",
    "        return (image, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4493 45\n"
     ]
    }
   ],
   "source": [
    "train_ds = tweetdataset(train, transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=100)\n",
    "print(len(train_ds), len(train_dl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1123 12\n"
     ]
    }
   ],
   "source": [
    "test_ds = tweetdataset(test, transform)\n",
    "test_dl = DataLoader(test_ds, batch_size=100)\n",
    "print(len(test_ds), len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweetscreenshot(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # onvolutional layers (3,16,32)\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size=(5, 5), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(5, 5), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
    "\n",
    "        # conected layers\n",
    "        self.fc1 = nn.Linear(in_features= 64 * 6 * 6, out_features=500)\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=50)\n",
    "        self.fc3 = nn.Linear(in_features=50, out_features=2)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2)\n",
    "\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2)\n",
    "\n",
    "        X = F.relu(self.conv3(X))\n",
    "        X = F.max_pool2d(X, 2)\n",
    "\n",
    "        X = X.view(X.shape[0], -1)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tweetscreenshot()\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "epoches = 3\n",
    "start = time.time()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/William/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................\n",
      " --- Epoch: 0, train loss: 0.2804, train acc: 0.8952, time: 116.34352993965149\n",
      "Epoch: 0, test loss: 0.1354, test acc: 0.9397, time: 139.21007204055786\n",
      "\n",
      ".........."
     ]
    }
   ],
   "source": [
    "for epoch in range(epoches):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "\n",
    "    for X, y in train_dl:\n",
    "        preds = model(X)\n",
    "        loss = loss_fn(preds, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        accuracy = ((preds.argmax(dim=1) == y).float().mean())\n",
    "        epoch_accuracy += accuracy\n",
    "        epoch_loss += loss\n",
    "        print('.', end='', flush=True)\n",
    "\n",
    "    epoch_accuracy = epoch_accuracy/len(train_dl)\n",
    "    accuracies.append(epoch_accuracy)\n",
    "    epoch_loss = epoch_loss / len(train_dl)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    print(\"\\n --- Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, time: {}\".format(epoch, epoch_loss, epoch_accuracy, time.time() - start))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        test_epoch_loss = 0\n",
    "        test_epoch_accuracy = 0\n",
    "\n",
    "        for test_X, test_y in test_dl:\n",
    "\n",
    "            test_preds = model(test_X)\n",
    "            test_loss = loss_fn(test_preds, test_y)\n",
    "\n",
    "            test_epoch_loss += test_loss            \n",
    "            test_accuracy = ((test_preds.argmax(dim=1) == test_y).float().mean())\n",
    "            test_epoch_accuracy += test_accuracy\n",
    "\n",
    "        test_epoch_accuracy = test_epoch_accuracy/len(test_dl)\n",
    "        test_epoch_loss = test_epoch_loss / len(test_dl)\n",
    "\n",
    "        print(\"Epoch: {}, test loss: {:.4f}, test acc: {:.4f}, time: {}\\n\".format(epoch, test_epoch_loss, test_epoch_accuracy, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = os.listdir('/Users/yao/Desktop/detection/test_set/')\n",
    "test_files = list(filter(lambda x: x != 'test_set', test_files))\n",
    "def test_path(p): return f\"/Users/yao/Desktop/detection/test_set/{p}\"\n",
    "test_files = list(map(test_path, test_files))\n",
    "\n",
    "class Test(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        super().__init__()\n",
    "        self.paths = image_paths\n",
    "        self.len = len(self.paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return self.len\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        path = self.paths[index]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        fileid = path.split('/')[-1].split('.')[0]\n",
    "        return (image, fileid)\n",
    "\n",
    "test_ds = Test(test_files, transform)\n",
    "test_dl = DataLoader(test_ds, batch_size=100)\n",
    "len(test_ds), len(test_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenshot_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, fileid in test_dl:\n",
    "        preds = model(X)\n",
    "        preds_list = F.softmax(preds, dim=1)[:, 1].tolist()\n",
    "        screenshot_probs += list(zip(list(fileid), preds_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some images\n",
    "for img, probs in zip(test_files[:5], screenshot_probs[:5]):\n",
    "    pil_im = Image.open(img, 'r')\n",
    "    label = \"non-screenshot\" if probs[1] > 0.5 else \"screenshot\"\n",
    "    title = \"prob of non-screenshot: \" + str(probs[1]) + \" Classified as: \" + label\n",
    "    plt.figure()\n",
    "    plt.imshow(pil_im)\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Screenshot', 'Non-Screenshot']\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_files, screenshot_probs, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
